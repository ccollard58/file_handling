{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34266644",
   "metadata": {},
   "source": [
    "# Document Category Analysis with AI/LLM ü§ñüìÑ\n",
    "\n",
    "## Educational Demo: Intelligent Document Organization\n",
    "\n",
    "**Welcome to this interactive demonstration!** This notebook shows how we can use Large Language Models (LLMs) to automatically analyze and categorize documents.\n",
    "\n",
    "### What You'll Learn:\n",
    "1. **OCR & Text Extraction** - How to extract text from images and PDFs\n",
    "2. **LLM Document Analysis** - Using AI to understand document content\n",
    "3. **Intelligent Categorization** - Automatically grouping similar documents\n",
    "4. **Data Visualization** - Creating charts and graphs to understand results\n",
    "5. **Knowledge Graphs** - Visualizing relationships between documents\n",
    "\n",
    "### Real-World Applications:\n",
    "- üìä **Business**: Organize invoices, contracts, reports\n",
    "- üè• **Healthcare**: Categorize medical records, prescriptions\n",
    "- üéì **Education**: Sort research papers, assignments\n",
    "- üè† **Personal**: Organize family documents, receipts, photos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d52a8",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports üîß\n",
    "\n",
    "First, let's import all the libraries we need and configure our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade pip\n",
    "%pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "\n",
    "# Project-specific imports\n",
    "from llm_analyzer import LLMAnalyzer\n",
    "from document_processor import DocumentProcessor\n",
    "\n",
    "# Configure visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a20214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced error handling for document processing\n",
    "def safe_file_analysis(analyzer, file_path):\n",
    "    \"\"\"Safely analyze a file with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        result = analyzer.analyze_file(file_path)\n",
    "        return result, None\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        # Check for common error types and provide helpful messages\n",
    "        if \"invalid pdf header\" in error_msg.lower():\n",
    "            return None, \"Invalid PDF file format - file may be corrupted or not a real PDF\"\n",
    "        elif \"cannot identify image file\" in error_msg.lower():\n",
    "            return None, \"Invalid image file format - file may be corrupted or not a real image\"\n",
    "        elif \"stream has ended unexpectedly\" in error_msg.lower():\n",
    "            return None, \"PDF file appears to be corrupted or incomplete\"\n",
    "        elif \"tesseract\" in error_msg.lower():\n",
    "            return None, \"OCR engine (Tesseract) error - check installation\"\n",
    "        else:\n",
    "            return None, f\"Unexpected error: {error_msg}\"\n",
    "\n",
    "print(\"üõ°Ô∏è Enhanced error handling functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aad46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for the notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('category_analysis_demo.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a logger for our demo\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"üöÄ Document Category Analyzer Demo Started\")\n",
    "\n",
    "# Suppress verbose HTTP request logs from httpx/urllib3\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"üìù Logging configured - check 'category_analysis_demo.log' for detailed logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c75d4",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions üõ†Ô∏è\n",
    "\n",
    "Let's define the core functions that will help us analyze documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging for the notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('category_analysis_demo.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a logger for our demo\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"üöÄ Document Category Analyzer Demo Started\")\n",
    "\n",
    "print(\"üìù Logging configured - check 'category_analysis_demo.log' for detailed logs\")\n",
    "print(\"üí° Note: This demo uses .txt files for reliable text extraction\")\n",
    "print(\"üîß In production, you'd use real PDFs and images with proper OCR\")\n",
    "\n",
    "class DocumentCategoryAnalyzer:\n",
    "    \"\"\"\n",
    "    An intelligent document categorization system that uses LLMs to:\n",
    "    1. Extract text from documents (PDFs, images)\n",
    "    2. Analyze content using AI\n",
    "    3. Suggest optimal category structures\n",
    "    4. Visualize results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_dir, sample_size=1000):\n",
    "        \"\"\"Initialize the analyzer with source directory and sample size.\"\"\"\n",
    "        self.source_dir = source_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.llm_analyzer = LLMAnalyzer()\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        \n",
    "        # Supported file types\n",
    "        self.supported_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.tif', '.tiff', '.docx', '.doc', '.txt', '.bmp', '.csv', '.xlsx', '.pptx', '.ppt', '.gif']\n",
    "        \n",
    "        # Initial category system\n",
    "        self.existing_categories = [\"Medical Documents\", \"Receipts\", \"Contracts\", \"Photographs\", \"Other\"]\n",
    "        \n",
    "        # Use the same LLM instance for consistency\n",
    "        self.llm = self.llm_analyzer.llm\n",
    "        \n",
    "        print(f\"ü§ñ Analyzer initialized for directory: {source_dir}\")\n",
    "        print(f\"üìä Sample size: {sample_size} files\")\n",
    "        print(f\"üìÅ Supported formats: {', '.join(self.supported_extensions)}\")\n",
    "    \n",
    "    def get_all_files(self):\n",
    "        \"\"\"Discover all supported files in the source directory.\"\"\"\n",
    "        all_files = []\n",
    "        file_stats = defaultdict(int)\n",
    "        \n",
    "        for root, _, files in os.walk(self.source_dir):\n",
    "            for file in files:\n",
    "                file_ext = os.path.splitext(file)[1].lower()\n",
    "                if file_ext in self.supported_extensions:\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    all_files.append(full_path)\n",
    "                    file_stats[file_ext] += 1\n",
    "        \n",
    "        logger.info(f\"Found {len(all_files)} documents in corpus\")\n",
    "        return all_files, dict(file_stats)\n",
    "    \n",
    "    def select_sample(self, all_files):\n",
    "        \"\"\"Select a representative sample using stratified sampling.\"\"\"\n",
    "        if len(all_files) <= self.sample_size:\n",
    "            return all_files\n",
    "        \n",
    "        # Stratified sampling by file type and folder\n",
    "        samples_by_ext = defaultdict(list)\n",
    "        samples_by_folder = defaultdict(list)\n",
    "        \n",
    "        for file_path in all_files:\n",
    "            ext = os.path.splitext(file_path)[1].lower()\n",
    "            folder = os.path.basename(os.path.dirname(file_path))\n",
    "            \n",
    "            samples_by_ext[ext].append(file_path)\n",
    "            samples_by_folder[folder].append(file_path)\n",
    "        \n",
    "        sample = []\n",
    "        \n",
    "        # Ensure representation from each file type\n",
    "        for ext, files in samples_by_ext.items():\n",
    "            sample.append(random.choice(files))\n",
    "        \n",
    "        # Ensure representation from each folder\n",
    "        for folder, files in samples_by_folder.items():\n",
    "            if not any(f in sample for f in files):\n",
    "                sample.append(random.choice(files))\n",
    "        \n",
    "        # Fill remaining slots randomly\n",
    "        remaining_files = [f for f in all_files if f not in sample]\n",
    "        random.shuffle(remaining_files)\n",
    "        sample.extend(remaining_files[:self.sample_size - len(sample)])\n",
    "        \n",
    "        logger.info(f\"Selected {len(sample)} files for analysis\")\n",
    "        return sample\n",
    "    \n",
    "    def analyze_file(self, file_path):\n",
    "        \"\"\"Analyze a single document using LLM.\"\"\"\n",
    "        try:\n",
    "            filename = os.path.basename(file_path)\n",
    "            creation_time = datetime.fromtimestamp(os.path.getctime(file_path))\n",
    "            \n",
    "            # Extract text using OCR/parsing\n",
    "            text = self.document_processor.extract_text(file_path)\n",
    "            \n",
    "            # Analyze with LLM\n",
    "            result = self.llm_analyzer.analyze_document(text, filename, creation_time, file_path)\n",
    "            \n",
    "            # Add metadata\n",
    "            result[\"file_path\"] = file_path\n",
    "            result[\"parent_folder\"] = os.path.basename(os.path.dirname(file_path))\n",
    "            result[\"file_size\"] = os.path.getsize(file_path)\n",
    "            result[\"text_length\"] = len(text) if text else 0\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "print(\"‚úÖ DocumentCategoryAnalyzer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d43d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_distribution_chart(file_stats):\n",
    "    \"\"\"Create a pie chart showing file type distribution.\"\"\"\n",
    "    fig = px.pie(\n",
    "        values=list(file_stats.values()),\n",
    "        names=list(file_stats.keys()),\n",
    "        title=\"üìÅ File Type Distribution in Dataset\",\n",
    "        color_discrete_sequence=px.colors.qualitative.Set3\n",
    "    )\n",
    "    fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "    return fig\n",
    "\n",
    "def create_category_distribution_chart(categories):\n",
    "    \"\"\"Create a bar chart showing category distribution, sorted by count.\"\"\"\n",
    "    # Sort categories by count (descending)\n",
    "    sorted_categories = dict(sorted(categories.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    fig = px.bar(\n",
    "        x=list(sorted_categories.keys()),\n",
    "        y=list(sorted_categories.values()),\n",
    "        title=\"üìä Document Categories Distribution\",\n",
    "        labels={'x': 'Category', 'y': 'Number of Documents'},\n",
    "        color=list(sorted_categories.values()),\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    fig.update_layout(showlegend=False)\n",
    "    return fig\n",
    "\n",
    "def create_folder_category_heatmap(folder_categories):\n",
    "    \"\"\"Create a heatmap showing folder-category relationships.\"\"\"\n",
    "    # Convert to DataFrame for heatmap\n",
    "    all_categories = set()\n",
    "    for cats in folder_categories.values():\n",
    "        all_categories.update(cats.keys())\n",
    "    \n",
    "    # Sort categories by total count across all folders\n",
    "    category_totals = defaultdict(int)\n",
    "    for cats in folder_categories.values():\n",
    "        for cat, count in cats.items():\n",
    "            category_totals[cat] += count\n",
    "    \n",
    "    sorted_categories = sorted(all_categories, key=lambda x: category_totals[x], reverse=True)\n",
    "    \n",
    "    # Sort folders by total document count\n",
    "    folder_totals = {folder: sum(cats.values()) for folder, cats in folder_categories.items()}\n",
    "    sorted_folders = sorted(folder_categories.keys(), key=lambda x: folder_totals[x], reverse=True)\n",
    "    \n",
    "    data = []\n",
    "    for folder in sorted_folders:\n",
    "        cats = folder_categories[folder]\n",
    "        row = [cats.get(cat, 0) for cat in sorted_categories]\n",
    "        data.append(row)\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=data,\n",
    "        x=sorted_categories,\n",
    "        y=sorted_folders,\n",
    "        colorscale='Blues',\n",
    "        showscale=True\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üóÇÔ∏è Folder-Category Relationship Heatmap\",\n",
    "        xaxis_title=\"Categories\",\n",
    "        yaxis_title=\"Folders\"\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def create_document_network_graph(analysis_results):\n",
    "    \"\"\"Create a network graph showing document relationships.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes for categories\n",
    "    categories = set(result['category'] for result in analysis_results)\n",
    "    for cat in categories:\n",
    "        G.add_node(cat, node_type='category', size=20)\n",
    "    \n",
    "    # Add nodes for folders\n",
    "    folders = set(result['parent_folder'] for result in analysis_results)\n",
    "    for folder in folders:\n",
    "        G.add_node(folder, node_type='folder', size=15)\n",
    "    \n",
    "    # Add edges between folders and categories\n",
    "    folder_category_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for result in analysis_results:\n",
    "        folder_category_counts[result['parent_folder']][result['category']] += 1\n",
    "    \n",
    "    for folder, cats in folder_category_counts.items():\n",
    "        for cat, count in cats.items():\n",
    "            G.add_edge(folder, cat, weight=count)\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    # Prepare traces\n",
    "    edge_trace = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_trace.append(go.Scatter(\n",
    "            x=[x0, x1, None], y=[y0, y1, None],\n",
    "            mode='lines',\n",
    "            line=dict(width=0.5, color='rgba(125,125,125,0.5)'),\n",
    "            hoverinfo='none',\n",
    "            showlegend=False\n",
    "        ))\n",
    "    \n",
    "    # Node traces\n",
    "    category_nodes = [n for n in G.nodes() if n in categories]\n",
    "    folder_nodes = [n for n in G.nodes() if n in folders]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add edges\n",
    "    for trace in edge_trace:\n",
    "        fig.add_trace(trace)\n",
    "    \n",
    "    # Add category nodes\n",
    "    if category_nodes:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[pos[node][0] for node in category_nodes],\n",
    "            y=[pos[node][1] for node in category_nodes],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=20, color='lightblue', line=dict(width=2, color='darkblue')),\n",
    "            text=category_nodes,\n",
    "            textposition=\"middle center\",\n",
    "            name=\"Categories\",\n",
    "            hovertemplate=\"Category: %{text}<extra></extra>\"\n",
    "        ))\n",
    "    \n",
    "    # Add folder nodes\n",
    "    if folder_nodes:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[pos[node][0] for node in folder_nodes],\n",
    "            y=[pos[node][1] for node in folder_nodes],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=15, color='lightgreen', line=dict(width=2, color='darkgreen')),\n",
    "            text=folder_nodes,\n",
    "            textposition=\"middle center\",\n",
    "            name=\"Folders\",\n",
    "            hovertemplate=\"Folder: %{text}<extra></extra>\"\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üï∏Ô∏è Document Organization Network\",\n",
    "        showlegend=True,\n",
    "        hovermode='closest',\n",
    "        margin=dict(b=20,l=5,r=5,t=40),\n",
    "        annotations=[\n",
    "            dict(\n",
    "                text=\"Network showing relationships between folders and document categories\",\n",
    "                showarrow=False,\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                x=0.005, y=-0.002,\n",
    "                xanchor='left', yanchor='bottom',\n",
    "                font=dict(color=\"grey\", size=12)\n",
    "            )\n",
    "        ],\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"‚úÖ Visualization helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5b8ef",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data üìä\n",
    "\n",
    "Now let's initialize our analyzer and explore the document collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SOURCE_DIR = \"e:/dropbox/admin/scanned documents\"\n",
    "SAMPLE_SIZE = 1000  \n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = DocumentCategoryAnalyzer(SOURCE_DIR, SAMPLE_SIZE)\n",
    "\n",
    "# Discover all files\n",
    "all_files, file_stats = analyzer.get_all_files()\n",
    "\n",
    "print(f\"üìÅ Total files found: {len(all_files)}\")\n",
    "print(f\"üìä File type breakdown: {file_stats}\")\n",
    "print(\"\\nüìù Sample files:\")\n",
    "for i, file_path in enumerate(all_files[:5]):\n",
    "    print(f\"  {i+1}. {os.path.basename(file_path)} ({os.path.splitext(file_path)[1]})\")\n",
    "    \n",
    "if len(all_files) > 5:\n",
    "    print(f\"  ... and {len(all_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ae62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize file type distribution\n",
    "if file_stats:\n",
    "    fig = create_file_distribution_chart(file_stats)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No files found to visualize\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "file_df = pd.DataFrame([\n",
    "    {\n",
    "        'File': os.path.basename(file_path),\n",
    "        'Extension': os.path.splitext(file_path)[1],\n",
    "        'Folder': os.path.basename(os.path.dirname(file_path)),\n",
    "        'Size (KB)': round(os.path.getsize(file_path) / 1024, 2)\n",
    "    }\n",
    "    for file_path in all_files\n",
    "])\n",
    "\n",
    "print(\"\\nüìà File Statistics:\")\n",
    "print(file_df.describe())\n",
    "print(\"\\nüìã First few files:\")\n",
    "print(file_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a8ee4",
   "metadata": {},
   "source": [
    "## 4. Select Sample Files üéØ\n",
    "\n",
    "For efficiency, we'll analyze a representative sample of files using stratified sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd441f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select representative sample\n",
    "sample_files = analyzer.select_sample(all_files)\n",
    "\n",
    "print(f\"üéØ Selected {len(sample_files)} files for analysis:\")\n",
    "print(\"\\nüìã Sample Details:\")\n",
    "\n",
    "sample_data = []\n",
    "for i, file_path in enumerate(sample_files):\n",
    "    file_info = {\n",
    "        'Index': i + 1,\n",
    "        'Filename': os.path.basename(file_path),\n",
    "        'Extension': os.path.splitext(file_path)[1],\n",
    "        'Folder': os.path.basename(os.path.dirname(file_path)),\n",
    "        'Size (KB)': round(os.path.getsize(file_path) / 1024, 2),\n",
    "        'Full Path': file_path\n",
    "    }\n",
    "    sample_data.append(file_info)\n",
    "    \n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "print(sample_df[['Index', 'Filename', 'Extension', 'Folder', 'Size (KB)']].to_string(index=False))\n",
    "\n",
    "# Show sampling strategy effectiveness\n",
    "print(\"\\nüîç Sampling Strategy Results:\")\n",
    "print(f\"üìä Extensions represented: {sample_df['Extension'].nunique()} out of {len(file_stats)}\")\n",
    "print(f\"üìÅ Folders represented: {sample_df['Folder'].nunique()}\")\n",
    "print(f\"üìà Coverage: {len(sample_files)} / {len(all_files)} files ({round(100*len(sample_files)/len(all_files), 1)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5e7ed",
   "metadata": {},
   "source": [
    "## 5. Analyze Files with AI ü§ñ\n",
    "\n",
    "Now comes the exciting part - using AI to analyze and categorize our documents!\n",
    "\n",
    "### How it works:\n",
    "1. **Text Extraction**: OCR for images, parsing for PDFs\n",
    "2. **LLM Analysis**: AI reads and understands content\n",
    "3. **Classification**: Assigns categories based on content\n",
    "4. **Metadata Extraction**: Finds dates, people, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2dedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each file in the sample\n",
    "analysis_results = []\n",
    "processing_stats = {\n",
    "    'successful': 0,\n",
    "    'failed': 0,\n",
    "    'total_time': 0.0,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "print(\"üîÑ Starting document analysis...\\n\")\n",
    "\n",
    "for i, file_path in enumerate(sample_files):\n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        print(f\"üìÑ Analyzing {i+1}/{len(sample_files)}: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        result = analyzer.analyze_file(file_path)\n",
    "        \n",
    "        if result:\n",
    "            analysis_results.append(result)\n",
    "            processing_stats['successful'] += 1\n",
    "            \n",
    "            # Show intermediate results\n",
    "            print(f\"  ‚úÖ Category: {result['category']}\")\n",
    "            print(f\"  üìù Description: {result['description']}\")\n",
    "            print(f\"  üë§ Identity: {result['identity']}\")\n",
    "            print(f\"  üìÖ Date: {result['date']}\")\n",
    "        else:\n",
    "            processing_stats['failed'] += 1\n",
    "            print(f\"  ‚ùå Analysis failed\")\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        processing_stats['total_time'] += duration\n",
    "        print(f\"  ‚è±Ô∏è Time: {duration:.1f}s\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_stats['failed'] += 1\n",
    "        processing_stats['errors'].append(f\"{os.path.basename(file_path)}: {str(e)}\")\n",
    "        print(f\"  ‚ùå Error: {str(e)}\\n\")\n",
    "        print(f\"  ‚ùå Error: {str(e)}\\n\")\n",
    "\n",
    "print(\"\\nüìä Analysis Summary:\")\n",
    "print(f\"‚úÖ Successful: {processing_stats['successful']}\")\n",
    "print(f\"‚ùå Failed: {processing_stats['failed']}\")\n",
    "print(f\"‚è±Ô∏è Total time: {processing_stats['total_time']:.1f}s\")\n",
    "print(f\"üìà Average time per file: {processing_stats['total_time']/len(sample_files):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defdee93",
   "metadata": {},
   "source": [
    "## 4.5. Document Gallery Preview üñºÔ∏è\n",
    "\n",
    "Before we visualize results, let's create a gallery showing thumbnails and the AI‚Äêassigned category for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import fitz  # PyMuPDF for PDF thumbnails\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def create_thumbnail(file_path, max_size=(150,200)):\n",
    "    \"\"\"Create thumbnail for image/PDF/text files.\"\"\"\n",
    "    # For images\n",
    "    if file_path.lower().endswith(('png','jpg','jpeg','gif','bmp','tif','tiff')):\n",
    "        img = Image.open(file_path)\n",
    "        img.thumbnail(max_size, Image.LANCZOS)\n",
    "        return img\n",
    "    # For PDFs\n",
    "    elif file_path.lower().endswith('pdf'):\n",
    "        doc = fitz.open(file_path)\n",
    "        page = doc[0]\n",
    "        pix = page.get_pixmap()\n",
    "        # Handle different color modes properly\n",
    "        if pix.n == 4:  # RGBA\n",
    "            img = Image.frombytes(\"RGBA\", [pix.width, pix.height], pix.samples)\n",
    "            img = img.convert(\"RGB\")  # Convert RGBA to RGB\n",
    "        else:  # RGB\n",
    "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        img.thumbnail(max_size, Image.LANCZOS)\n",
    "        doc.close()  # Close the document to free memory\n",
    "        return img\n",
    "    # For other file types, create a placeholder image\n",
    "    else:\n",
    "        return create_placeholder_thumbnail(max_size, os.path.splitext(file_path)[1])\n",
    "    \n",
    "def create_text_thumbnail(file_path, max_size=(150,200)):\n",
    "    \"\"\"Create a simple text-based thumbnail for non-image files.\"\"\"\n",
    "    try:\n",
    "        # Generate a placeholder image with file type and name\n",
    "        img = Image.new('RGB', max_size, color = (255, 255, 255))\n",
    "        d = ImageDraw.Draw(img)\n",
    "        text = os.path.basename(file_path)\n",
    "        text = text if len(text) <= 20 else text[:17] + '...'  # Truncate long names\n",
    "        d.text((10,10), text, fill=(0,0,0))\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating text thumbnail for {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_placeholder_thumbnail(max_size=(150,200), file_ext=\"\"):\n",
    "    \"\"\"Create a placeholder thumbnail image.\"\"\"\n",
    "    try:\n",
    "        # Generate a simple placeholder image with file extension\n",
    "        img = Image.new('RGB', max_size, color = (220, 220, 220))\n",
    "        d = ImageDraw.Draw(img)\n",
    "        text = file_ext.upper() if file_ext else \"FILE\"\n",
    "        d.text((10,10), text, fill=(0,0,0))\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating placeholder thumbnail: {e}\")\n",
    "        return None\n",
    "\n",
    "def image_to_base64(img):\n",
    "    buffer = BytesIO()\n",
    "    img.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def format_file_size(size_bytes):\n",
    "    \"\"\"Human-readable file size.\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.1f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "    return f\"{size_bytes:.1f} TB\"\n",
    "\n",
    "# Build gallery items using AI-assigned categories\n",
    "gallery_items = []\n",
    "for idx, fp in enumerate(sample_files, start=1):\n",
    "    category = next((r['category'] for r in analysis_results if r['file_path']==fp), \"Unknown\")\n",
    "    thumb = create_thumbnail(fp)\n",
    "    thumb_b64 = \"data:image/png;base64,\" + image_to_base64(thumb)\n",
    "    gallery_items.append({\n",
    "        'index': idx,\n",
    "        'filename': os.path.basename(fp),\n",
    "        'size': format_file_size(os.path.getsize(fp)),\n",
    "        'folder': os.path.basename(os.path.dirname(fp)),\n",
    "        'category': category,\n",
    "        'thumbnail': thumb_b64\n",
    "    })\n",
    "\n",
    "# Render HTML gallery\n",
    "html = \"\"\"\n",
    "<style>\n",
    ".gallery-container {\n",
    "    display: grid;\n",
    "    grid-template-columns: repeat(auto-fill, minmax(160px, 1fr));\n",
    "    gap: 10px;\n",
    "}\n",
    ".gallery-item {\n",
    "    background: #fff;\n",
    "    border: 1px solid #ddd;\n",
    "    border-radius: 8px;\n",
    "    overflow: hidden;\n",
    "    padding: 8px;\n",
    "    text-align: center;\n",
    "}\n",
    ".thumbnail {\n",
    "    width: 100%;\n",
    "    height: auto;\n",
    "    border-radius: 4px;\n",
    "}\n",
    ".item-info {\n",
    "    margin-top: 8px;\n",
    "    font-size: 14px;\n",
    "}\n",
    ".filename {\n",
    "    font-weight: bold;\n",
    "}\n",
    ".category {\n",
    "    color: #555;\n",
    "}\n",
    ".file-details {\n",
    "    font-size: 12px;\n",
    "    color: #777;\n",
    "}\n",
    "</style>\n",
    "<div class=\"gallery-container\">\n",
    "\"\"\"\n",
    "for item in gallery_items:\n",
    "    html += f\"\"\"\n",
    "    <div class=\"gallery-item\">\n",
    "        <img src=\"{item['thumbnail']}\" class=\"thumbnail\">\n",
    "        <div class=\"item-info\">\n",
    "            <div class=\"filename\">üìÑ {item['filename']}</div>\n",
    "            <div class=\"category\">{item['category']}</div>\n",
    "            <div class=\"file-details\">üíæ {item['size']} | üìÇ {item['folder']}</div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "html += \"</div>\"\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ddd38",
   "metadata": {},
   "source": [
    "### üéì What Just Happened?\n",
    "\n",
    "The AI system just performed several complex tasks:\n",
    "\n",
    "1. **üìñ Text Extraction**: Read the content from each document\n",
    "2. **üß† Content Understanding**: Used the LLM to understand what each document is about\n",
    "3. **üè∑Ô∏è Categorization**: Assigned appropriate categories based on content\n",
    "4. **üë§ Identity Detection**: Looked for person names (Chuck, Colleen) in the documents\n",
    "5. **üìÖ Date Extraction**: Found relevant dates in the document content\n",
    "6. **üìù Description Generation**: Created brief, descriptive titles\n",
    "\n",
    "**Key AI/ML Concepts Demonstrated:**\n",
    "- **Natural Language Processing (NLP)**: Understanding human-readable text\n",
    "- **Named Entity Recognition (NER)**: Identifying people, dates, organizations\n",
    "- **Text Classification**: Automatically categorizing documents\n",
    "- **Information Extraction**: Pulling structured data from unstructured text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416dbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive results DataFrame\n",
    "if analysis_results:\n",
    "    results_df = pd.DataFrame(analysis_results)\n",
    "    \n",
    "    print(\"\\nüìã Analysis Results Summary:\")\n",
    "    print(results_df[['description', 'category', 'identity', 'date', 'parent_folder']].to_string(index=False))\n",
    "    \n",
    "    # Category distribution\n",
    "    category_counts = Counter(result['category'] for result in analysis_results)\n",
    "    print(f\"\\nüìä Category Distribution:\")\n",
    "    for category, count in category_counts.most_common():\n",
    "        print(f\"  {category}: {count} documents\")\n",
    "    \n",
    "    # Identity distribution\n",
    "    identity_counts = Counter(result['identity'] for result in analysis_results)\n",
    "    print(f\"\\nüë§ Identity Distribution:\")\n",
    "    for identity, count in identity_counts.most_common():\n",
    "        print(f\"  {identity}: {count} documents\")\n",
    "    \n",
    "    # Folder analysis\n",
    "    folder_counts = Counter(result['parent_folder'] for result in analysis_results)\n",
    "    print(f\"\\nüìÅ Folder Distribution:\")\n",
    "    for folder, count in folder_counts.most_common():\n",
    "        print(f\"  {folder}: {count} documents\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No analysis results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ed889",
   "metadata": {},
   "source": [
    "## 6. AI-Generated Category Suggestions üí°\n",
    "\n",
    "Based on the document analysis, let's ask the AI to suggest an improved categorization system:\n",
    "\n",
    "# Analyze each file in the sample with enhanced error handling\n",
    "analysis_results = []\n",
    "processing_stats = {\n",
    "    'successful': 0,\n",
    "    'failed': 0,\n",
    "    'total_time': 0,\n",
    "    'errors': []\n",
    "}\n",
    "\n",
    "print(\"üîÑ Starting document analysis...\\n\")\n",
    "\n",
    "for i, file_path in enumerate(sample_files):\n",
    "    start_time = datetime.now()\n",
    "    print(f\"üìÑ Analyzing {i+1}/{len(sample_files)}: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # Use safe analysis function\n",
    "    result, error = safe_file_analysis(analyzer, file_path)\n",
    "    \n",
    "    if result:\n",
    "        analysis_results.append(result)\n",
    "        processing_stats['successful'] += 1\n",
    "        \n",
    "        # Show intermediate results\n",
    "        print(f\"  ‚úÖ Category: {result['category']}\")\n",
    "        print(f\"  üìù Description: {result['description']}\")\n",
    "        print(f\"  üë§ Identity: {result['identity']}\")\n",
    "        print(f\"  üìÖ Date: {result['date']}\")\n",
    "        print(f\"  üìä Text Length: {result.get('text_length', 0)} characters\")\n",
    "    else:\n",
    "        processing_stats['failed'] += 1\n",
    "        processing_stats['errors'].append(f\"{os.path.basename(file_path)}: {error}\")\n",
    "        print(f\"  ‚ùå Analysis failed: {error}\")\n",
    "        \n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    processing_stats['total_time'] += duration\n",
    "    print(f\"  ‚è±Ô∏è Time: {duration:.1f}s\\n\")\n",
    "\n",
    "print(\"\\nüìä Analysis Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"‚úÖ Successful: {processing_stats['successful']}\")\n",
    "print(f\"‚ùå Failed: {processing_stats['failed']}\")\n",
    "print(f\"‚è±Ô∏è Total time: {processing_stats['total_time']:.1f}s\")\n",
    "print(f\"üìà Average time per file: {processing_stats['total_time']/len(sample_files):.1f}s\")\n",
    "print(f\"üíØ Success rate: {(processing_stats['successful']/len(sample_files)*100):.1f}%\")\n",
    "\n",
    "if processing_stats['errors']:\n",
    "    print(f\"\\n‚ö†Ô∏è Errors encountered:\")\n",
    "    for error in processing_stats['errors']:\n",
    "        print(f\"   ‚Ä¢ {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_categories(analyzer, analysis_results):\n",
    "    \"\"\"Generate improved category suggestions using LLM.\"\"\"\n",
    "    if not analysis_results:\n",
    "        print(\"‚ö†Ô∏è No analysis results available for category suggestions\")\n",
    "        return None\n",
    "    \n",
    "    # Extract data for LLM prompt\n",
    "    current_categories = [result[\"category\"] for result in analysis_results]\n",
    "    descriptions = [result[\"description\"] for result in analysis_results]\n",
    "    parent_folders = [result[\"parent_folder\"] for result in analysis_results]\n",
    "    \n",
    "    # Create comprehensive prompt using ALL available data\n",
    "    prompt = f\"\"\"\n",
    "    I need to organize personal and family documents into meaningful categories.\n",
    "    \n",
    "    Currently, I'm using these categories:\n",
    "    {', '.join(analyzer.existing_categories)}\n",
    "    \n",
    "    I also have documents organized in these folders:\n",
    "    {', '.join(set(parent_folders))}\n",
    "    \n",
    "    Here are ALL the document descriptions from my collection:\n",
    "    {', '.join(descriptions)}\n",
    "    \n",
    "    The current categories assigned to these documents are:\n",
    "    {', '.join(current_categories)}\n",
    "    \n",
    "    Based on this information, please suggest:\n",
    "    1. An improved list of 10-20 categories that would be most useful for organizing and finding documents\n",
    "    2. A brief explanation of each category\n",
    "    3. Examples of what types of documents belong in each category\n",
    "    \n",
    "    Focus on categories that are:\n",
    "    - Mutually exclusive (minimal overlap)\n",
    "    - Collectively exhaustive (cover all document types)\n",
    "    - Intuitive for everyday use\n",
    "    - Useful for quickly finding specific documents\n",
    "    \n",
    "    Return your response as a JSON object with this structure:\n",
    "    {{\n",
    "        \"categories\": [\n",
    "            {{\n",
    "                \"name\": \"Category Name\",\n",
    "                \"description\": \"Brief description\",\n",
    "                \"examples\": [\"Example doc 1\", \"Example doc 2\"]\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"ü§î Asking AI for category suggestions...\")\n",
    "        response = analyzer.llm.invoke(prompt)\n",
    "        print(\"‚úÖ AI response received!\")\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        json_match = re.search(r'```json\\n(.*?)\\n```|{.*}', response, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1) if json_match.group(1) else json_match.group(0)\n",
    "            suggestions = json.loads(json_str)\n",
    "            return suggestions\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not parse JSON from AI response\")\n",
    "            print(f\"Raw response: {response[:500]}...\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting category suggestions: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Generate suggestions\n",
    "suggested_categories = suggest_categories(analyzer, analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the suggested categories\n",
    "if suggested_categories and \"categories\" in suggested_categories:\n",
    "    print(\"\\nüéØ AI-Suggested Document Categories:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, cat in enumerate(suggested_categories[\"categories\"], 1):\n",
    "        print(f\"\\n{i}. **{cat['name']}**\")\n",
    "        print(f\"   üìù Description: {cat['description']}\")\n",
    "        print(f\"   üìã Examples: {', '.join(cat['examples'])}\")\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    old_categories = analyzer.existing_categories\n",
    "    new_categories = [cat['name'] for cat in suggested_categories['categories']]\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Aspect': ['Number of Categories', 'Specificity', 'Coverage'],\n",
    "        'Original System': [len(old_categories), 'Basic', 'General'],\n",
    "        'AI-Suggested System': [len(new_categories), 'Detailed', 'Comprehensive']\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nüìä System Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Category evolution chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Original Categories',\n",
    "        x=old_categories,\n",
    "        y=[1] * len(old_categories),\n",
    "        marker_color='lightblue'\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='AI-Suggested Categories',\n",
    "        x=new_categories,\n",
    "        y=[1] * len(new_categories),\n",
    "        marker_color='lightgreen',\n",
    "        yaxis='y2'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"üìà Category System Evolution: Original vs AI-Suggested\",\n",
    "        xaxis_title=\"Categories\",\n",
    "        yaxis=dict(title=\"Original System\", side=\"left\"),\n",
    "        yaxis2=dict(title=\"AI-Suggested System\", side=\"right\", overlaying=\"y\"),\n",
    "        barmode='group',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No category suggestions available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1553725",
   "metadata": {},
   "source": [
    "## 7. Visualize Results üìäüé®\n",
    "\n",
    "Let's create comprehensive visualizations to understand our document analysis results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_results:\n",
    "    # 1. Category Distribution Chart\n",
    "    categories = Counter(result['category'] for result in analysis_results)\n",
    "    fig1 = create_category_distribution_chart(categories)\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. Folder-Category Relationship\n",
    "    folder_categories = defaultdict(lambda: defaultdict(int))\n",
    "    for result in analysis_results:\n",
    "        folder_categories[result['parent_folder']][result['category']] += 1\n",
    "    \n",
    "    # Convert to regular dict for JSON serialization\n",
    "    folder_categories = {k: dict(v) for k, v in folder_categories.items()}\n",
    "    \n",
    "    if len(folder_categories) > 1:  # Only show if multiple folders\n",
    "        fig2 = create_folder_category_heatmap(folder_categories)\n",
    "        fig2.show()\n",
    "    \n",
    "    # 3. Document Properties Analysis\n",
    "    properties_df = pd.DataFrame(analysis_results)\n",
    "    \n",
    "    # File size vs text length correlation\n",
    "    fig3 = px.scatter(\n",
    "        properties_df,\n",
    "        x='file_size',\n",
    "        y='text_length',\n",
    "        color='category',\n",
    "        title=\"üìÑ Document Size vs Text Content\",\n",
    "        labels={'file_size': 'File Size (bytes)', 'text_length': 'Extracted Text Length'},\n",
    "        hover_data=['description', 'parent_folder']\n",
    "    )\n",
    "    fig3.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if analysis_results and len(analysis_results) > 1:\n",
    "    # 4. Document Network Graph\n",
    "    print(\"üï∏Ô∏è Creating document organization network...\")\n",
    "    fig4 = create_document_network_graph(analysis_results)\n",
    "    fig4.show()\n",
    "    \n",
    "    # 5. Timeline Analysis (if dates available)\n",
    "    if analysis_results:\n",
    "        try:\n",
    "            # Parse and filter dates flexibly using pandas\n",
    "            valid_entries = []\n",
    "            for res in analysis_results:\n",
    "                dstr = res.get('date')\n",
    "                if not dstr or dstr == 'Unknown':\n",
    "                    continue\n",
    "                d = pd.to_datetime(dstr, errors='coerce')\n",
    "                if pd.isna(d):\n",
    "                    continue\n",
    "                # Filter out implausible dates\n",
    "                if d.year < 1900 or d > pd.Timestamp.now():\n",
    "                    continue\n",
    "                valid_entries.append({\n",
    "                    'Start': d,\n",
    "                    'End': d + pd.Timedelta(days=1),\n",
    "                    'Document': res['description'],\n",
    "                    'Category': res['category']\n",
    "                })\n",
    "            if valid_entries:\n",
    "                timeline_df = pd.DataFrame(valid_entries)\n",
    "                fig5 = px.timeline(\n",
    "                    timeline_df,\n",
    "                    x_start='Start',\n",
    "                    x_end='End',\n",
    "                    y='Document',\n",
    "                    color='Category',\n",
    "                    title=\"üìÖ Document Timeline\"\n",
    "                )\n",
    "                fig5.show()\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No valid dates for timeline\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not create timeline: {e}\")\n",
    "    \n",
    "    # 6. Advanced Analytics Dashboard\n",
    "    fig6 = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"Categories by Count\",\n",
    "            \"Identity Distribution\", \n",
    "            \"Folder Distribution\",\n",
    "            \"Text Length Distribution\"\n",
    "        ),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"histogram\"}]]\n",
    "    )\n",
    "    \n",
    "    # Category counts - sorted by count descending\n",
    "    cat_counts = Counter(r['category'] for r in analysis_results)\n",
    "    sorted_cat_counts = dict(cat_counts.most_common())\n",
    "    fig6.add_trace(\n",
    "        go.Bar(x=list(sorted_cat_counts.keys()), y=list(sorted_cat_counts.values()), name=\"Categories\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Identity pie - sorted by count descending\n",
    "    id_counts = Counter(r['identity'] for r in analysis_results)\n",
    "    sorted_id_counts = dict(id_counts.most_common())\n",
    "    fig6.add_trace(\n",
    "        go.Pie(labels=list(sorted_id_counts.keys()), values=list(sorted_id_counts.values()), name=\"Identity\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Folder distribution - sorted by count descending\n",
    "    folder_counts = Counter(r['parent_folder'] for r in analysis_results)\n",
    "    sorted_folder_counts = dict(folder_counts.most_common())\n",
    "    fig6.add_trace(\n",
    "        go.Bar(x=list(sorted_folder_counts.keys()), y=list(sorted_folder_counts.values()), name=\"Folders\"),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Text length histogram\n",
    "    text_lengths = [r.get('text_length', 0) for r in analysis_results]\n",
    "    fig6.add_trace(\n",
    "        go.Histogram(x=text_lengths, name=\"Text Length\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig6.update_layout(\n",
    "        title_text=\"üìä Comprehensive Document Analysis Dashboard\",\n",
    "        height=600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig6.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Insufficient data for network visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca9d26",
   "metadata": {},
   "source": [
    "## 8. Save and Display Results üíæüìã\n",
    "\n",
    "Finally, let's save our analysis results and create a comprehensive summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_results(analysis_results, suggested_categories, filename=\"category_analysis_demo_results.json\"):\n",
    "    \"\"\"Save all analysis results to a JSON file.\"\"\"\n",
    "    \n",
    "    if not analysis_results:\n",
    "        print(\"‚ö†Ô∏è No analysis results to save\")\n",
    "        return\n",
    "    \n",
    "    # Compute statistics\n",
    "    categories = Counter(result[\"category\"] for result in analysis_results)\n",
    "    folder_categories = defaultdict(list)\n",
    "    \n",
    "    for result in analysis_results:\n",
    "        folder_categories[result[\"parent_folder\"]].append(result[\"category\"])\n",
    "    \n",
    "    # Convert to regular dict for JSON serialization\n",
    "    folder_categories = {k: dict(Counter(v)) for k, v in folder_categories.items()}\n",
    "    \n",
    "    # Create comprehensive results object\n",
    "    results = {\n",
    "        \"analysis_metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"source_directory\": analyzer.source_dir,\n",
    "            \"total_files_found\": len(all_files),\n",
    "            \"files_analyzed\": len(analysis_results),\n",
    "            \"sample_size\": analyzer.sample_size,\n",
    "            \"processing_stats\": processing_stats\n",
    "        },\n",
    "        \"file_statistics\": {\n",
    "            \"file_type_distribution\": file_stats,\n",
    "            \"current_categories\": dict(categories),\n",
    "            \"folder_categories\": folder_categories\n",
    "        },\n",
    "        \"analysis_results\": analysis_results,\n",
    "        \"suggested_categories\": suggested_categories,\n",
    "        \"insights\": {\n",
    "            \"most_common_category\": categories.most_common(1)[0] if categories else None,\n",
    "            \"total_categories_found\": len(categories),\n",
    "            \"average_text_length\": sum(r.get('text_length', 0) for r in analysis_results) / len(analysis_results) if analysis_results else 0,\n",
    "            \"folders_analyzed\": len(folder_categories)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üíæ Results saved to: {filename}\")\n",
    "    return results\n",
    "\n",
    "# Save the results\n",
    "saved_results = save_analysis_results(analysis_results, suggested_categories)\n",
    "\n",
    "if saved_results:\n",
    "    print(\"\\nüìä Final Analysis Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    metadata = saved_results[\"analysis_metadata\"]\n",
    "    insights = saved_results[\"insights\"]\n",
    "    \n",
    "    print(f\"üìÖ Analysis Date: {metadata['timestamp'][:19]}\")\n",
    "    print(f\"üìÅ Source Directory: {metadata['source_directory']}\")\n",
    "    print(f\"üìÑ Files Found: {metadata['total_files_found']}\")\n",
    "    print(f\"üîç Files Analyzed: {metadata['files_analyzed']}\")\n",
    "    print(f\"‚úÖ Success Rate: {processing_stats['successful']}/{len(sample_files)} ({round(100*processing_stats['successful']/len(sample_files), 1)}%)\")\n",
    "    \n",
    "    if insights['most_common_category']:\n",
    "        print(f\"üìä Most Common Category: {insights['most_common_category'][0]} ({insights['most_common_category'][1]} documents)\")\n",
    "    \n",
    "    print(f\"üè∑Ô∏è Categories Discovered: {insights['total_categories_found']}\")\n",
    "    print(f\"üìù Average Text Length: {round(insights['average_text_length'], 1)} characters\")\n",
    "    print(f\"üìÇ Folders Analyzed: {insights['folders_analyzed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actionable recommendations\n",
    "print(\"\\nüí° Actionable Recommendations:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if analysis_results:\n",
    "    recommendations = []\n",
    "    \n",
    "    # Category-based recommendations\n",
    "    categories = Counter(result['category'] for result in analysis_results)\n",
    "    if len(categories) > 1:\n",
    "        recommendations.append(\"‚úÖ Document diversity detected - categorization system is working\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ö†Ô∏è Low category diversity - consider reviewing categorization criteria\")\n",
    "    \n",
    "    # Folder organization recommendations\n",
    "    folder_categories = defaultdict(set)\n",
    "    for result in analysis_results:\n",
    "        folder_categories[result['parent_folder']].add(result['category'])\n",
    "    \n",
    "    mixed_folders = {folder: cats for folder, cats in folder_categories.items() if len(cats) > 1}\n",
    "    if mixed_folders:\n",
    "        recommendations.append(f\"üìÅ {len(mixed_folders)} folders contain mixed categories - consider reorganization\")\n",
    "    \n",
    "    # Text extraction recommendations\n",
    "    avg_text_length = sum(r.get('text_length', 0) for r in analysis_results) / len(analysis_results)\n",
    "    if avg_text_length < 100:\n",
    "        recommendations.append(\"üìù Low average text extraction - consider OCR optimization\")\n",
    "    \n",
    "    # Identity detection recommendations\n",
    "    identities = Counter(result['identity'] for result in analysis_results)\n",
    "    unknown_ratio = identities.get('Unknown', 0) / len(analysis_results)\n",
    "    if unknown_ratio > 0.5:\n",
    "        recommendations.append(\"üë§ High unknown identity rate - consider improving name detection\")\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No analysis results available for recommendations\")\n",
    "\n",
    "print(\"\\nüéì Learning Outcomes Achieved:\")\n",
    "print(\"=\" * 35)\n",
    "learning_outcomes = [\n",
    "    \"‚úÖ Understanding OCR and text extraction from documents\",\n",
    "    \"‚úÖ Experience with Large Language Model (LLM) analysis\",\n",
    "    \"‚úÖ Knowledge of automated document categorization\",\n",
    "    \"‚úÖ Data visualization and interpretation skills\",\n",
    "    \"‚úÖ Practical application of AI in document management\",\n",
    "    \"‚úÖ Understanding of sampling strategies for large datasets\"\n",
    "]\n",
    "\n",
    "for outcome in learning_outcomes:\n",
    "    print(outcome)\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"=\" * 15)\n",
    "next_steps = [\n",
    "    \"1. Try with your own document collection\",\n",
    "    \"2. Experiment with different LLM models\",\n",
    "    \"3. Implement automated file organization\",\n",
    "    \"4. Add more sophisticated text preprocessing\",\n",
    "    \"5. Create a web interface for the system\",\n",
    "    \"6. Explore clustering algorithms for unsupervised categorization\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(step)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Document Category Analysis Demo Complete! üéâ\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
